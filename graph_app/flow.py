import sys
import os
import json
import datetime
from pathlib import Path
from langgraph.graph import StateGraph, END
from typing import TypedDict
from bson import ObjectId
from dotenv import load_dotenv

from utils.GPTClient import GPTClient
from utils.GeminiClient import GeminiClient
from modules.agents.ChatAgent import ChatAgent
from modules.agents.SubtopicGeneratorAgent import SubtopicGeneratorAgent
from modules.rag_module.SemanticChunkFilter import SemanticChunkFilter
from modules.rag_module.documents_processing.main_processor import EduMateDocumentProcessor
from modules.rag_module.data_chunking.processor import IntelligentVietnameseChunkingProcessor
from modules.rag_module.data_embedding.embedding_processor import VietnameseEmbeddingProcessor
from modules.rag_module.query_db.MongoDBClient import MongoDBClient
from modules.rag_module.DeepRetrieval import OptimizedDeepRetrieval, DeepRetrieval
from modules.lesson_plan.LessonPlanPipeline import LessonPlanPipeline
from modules.quiz_plan.QuizPipeline import QuizPipeline

load_dotenv()

# ========================= Helpers =========================
def clean_objectid(obj):
    if isinstance(obj, dict):
        return {k: clean_objectid(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_objectid(i) for i in obj]
    elif isinstance(obj, ObjectId):
        return str(obj)
    else:
        return obj


# ========================= State =========================
class FlowState(TypedDict):
    form_data: dict
    user_prompt: str
    subtopics: list
    db_chunks: list
    uploaded_chunks: list
    search_chunks: list
    all_chunks: list
    embedded_chunks: list
    filtered_chunks: list
    lesson_plan: dict
    quiz: dict
    output_path: str
    __skip__: bool
    __plan_done__: bool
    __quiz_done__: bool


# ========================= Nodes =========================
class PromptGenerator:
    def __init__(self, llm: GPTClient):
        self.agent = ChatAgent(llm)

    def __call__(self, state: FlowState):
        result = self.agent.run(mode="generate_prompt", form_data=state["form_data"])
        print("\nğŸ§  Prompt gá»‘c sinh tá»« form:")
        print(result)
        return {"user_prompt": result}


class SubtopicGenerator:
    def __init__(self, llm: GPTClient):
        self.agent = SubtopicGeneratorAgent(llm)

    def __call__(self, state: FlowState):
        prompt = state["user_prompt"]
        subtopics = self.agent.run(prompt)
        print(f"\nğŸ“Œ ÄÃ£ sinh {len(subtopics)} subtopics:")
        for i, topic in enumerate(subtopics, 1):
            print(f"   {i}. {topic}")
        return {"subtopics": subtopics}


class FileProcessor:
    def __init__(self):
        self.document_processor = EduMateDocumentProcessor.create_balanced()
        self.chunking_processor = IntelligentVietnameseChunkingProcessor(
            output_dir="temp_langgraph_chunking",
            min_quality=0.65
        )

    def __call__(self, state: FlowState):
        form = state["form_data"]
        files = form.get("files", [])
        if not files:
            print("\nKhÃ´ng cÃ³ file Ä‘Ã­nh kÃ¨m. Sáº½ gá»i agent truy váº¥n sau...")
            return {"__skip__": True, "search_chunks": [], "db_chunks": [], "all_chunks": []}

        all_standardized_chunks = []

        for file_path in files:
            print(f"\nğŸ“‚ Äang xá»­ lÃ½ file: {file_path}")
            try:
                file_path_obj = Path(file_path)

                # 1) Document processing
                doc_result = self.document_processor.process_file(file_path_obj)
                if not doc_result.success:
                    print(f"Document processing failed: {doc_result.error_message}")
                    continue

                # 2) Chunking (trÃªn file .md táº¡m)
                import tempfile
                with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as temp_file:
                    temp_file.write(doc_result.content)
                    temp_md_path = temp_file.name

                chunking_result = self.chunking_processor.run(
                    Path(temp_md_path),
                    strategy=None,
                    save_json=False,
                    print_report=False
                )
                chunks_data = chunking_result['result']['chunking_results']['chunks_data']

                # 3) Standardize
                for i, chunk_data in enumerate(chunks_data):
                    all_standardized_chunks.append({
                        "chunk_id": f"{Path(file_path).stem}_chunk_{i+1:02d}",
                        "content": chunk_data["content"],
                        "token_count": chunk_data.get("word_count", 0),
                        "method": chunk_data.get("chunking_strategy", "intelligent"),
                        "source_file": file_path,
                        "retrieved_from": "upload",
                        "char_count": chunk_data.get("char_count", 0),
                        "keywords": chunk_data.get("keywords", []),
                        "coherence_score": chunk_data.get("semantic_coherence_score", 0.0),
                        "completeness_score": chunk_data.get("completeness_score", 0.0),
                        "language_confidence": chunk_data.get("language_confidence", 0.0)
                    })

                print(f"âœ… Processed {file_path}: {len(chunks_data)} chunks created")

                try:
                    os.unlink(temp_md_path)
                except Exception:
                    pass

            except Exception as e:
                print(f"âš ï¸ Failed to process {file_path}: {e}")
                continue

        if not all_standardized_chunks:
            print("\nâŒ KhÃ´ng cÃ³ file nÃ o Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng")
            return {"__skip__": True, "search_chunks": [], "db_chunks": [], "all_chunks": []}

        print(f"\nğŸ‰ Tá»•ng cá»™ng Ä‘Ã£ xá»­ lÃ½: {len(all_standardized_chunks)} chunks tá»« {len(files)} files")
        return {
            "search_chunks": [],
            "db_chunks": [],
            "uploaded_chunks": all_standardized_chunks,
            "__skip__": False,
            "source_files": files,
            "processing_method": "new_intelligent",
        }


class AgentBasedRetrieval:
    def __init__(self):
        self.retriever = OptimizedDeepRetrieval()

    def __call__(self, state: FlowState):
        print("ğŸ§  [agent_retrieval] ÄÃƒ ÄÆ¯á»¢C Gá»ŒI")
        prompt = state.get("user_prompt", "")
        subtopics = state.get("subtopics", [])
        if not prompt or not subtopics:
            print("\nThiáº¿u prompt hoáº·c subtopics.")
            return {}

        chunks = self.retriever.retrieve(prompt, subtopics)
        db_chunks = [c for c in chunks if c.get("retrieved_from") == "db"]
        search_chunks = [c for c in chunks if c.get("retrieved_from") == "search"]
        all_chunks = db_chunks + search_chunks

        print(f"\nğŸ“¦ Tá»•ng cá»™ng: {len(all_chunks)} chunks (DB: {len(db_chunks)}, Search: {len(search_chunks)})")
        return {"db_chunks": db_chunks, "search_chunks": search_chunks, "all_chunks": all_chunks}


class EmbedAndStoreUploaded:
    def __call__(self, state: FlowState):
        chunks = state.get("uploaded_chunks", [])
        if not chunks:
            print("\n KhÃ´ng cÃ³ chunks Ä‘á»ƒ embedding (uploaded).")
            return {}
        file_path = state.get("source_file", "upload")
        source_name = os.path.basename(file_path)
        return embed_and_store_chunks(chunks, source_name)


class EmbedAndStoreSearched:
    def __call__(self, state: FlowState):
        chunks = state.get("search_chunks", [])
        if not chunks:
            print("\nâŒ KhÃ´ng cÃ³ chunks Ä‘á»ƒ embedding (searched).")
            return {}
        return embed_and_store_chunks(chunks, "web_search")


def embed_and_store_chunks(chunks, source_files):
    print("\nğŸ“„ Chunks:")
    print(json.dumps(chunks[:3], ensure_ascii=False, indent=2))
    if len(chunks) > 3:
        print(f"... (cÃ²n {len(chunks) - 3} chunks ná»¯a)\n")

    chunks_with_metadata = []
    for chunk in chunks:
        if "metadata" not in chunk:
            chunk["metadata"] = {}
        chunk["metadata"]["source_file"] = chunk.get("source_file", "unknown")
        chunk["metadata"]["all_source_files"] = source_files if isinstance(source_files, list) else [source_files]
        chunk["metadata"]["collection"] = "lectures"
        chunks_with_metadata.append(chunk)

    os.makedirs("temp_embedding", exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    temp_path = os.path.join("temp_embedding", f"temp_chunks_{timestamp}.json")

    with open(temp_path, "w", encoding="utf-8") as f:
        json.dump(chunks_with_metadata, f, ensure_ascii=False, indent=2)

    embedder = VietnameseEmbeddingProcessor()
    result = embedder.run(temp_path, save_results=False)
    embedded_chunks = result["chunks"]

    try:
        os.remove(temp_path)
    except Exception:
        pass

    db = MongoDBClient()
    db.insert_many("lectures", embedded_chunks)
    print(f"\nâœ… ÄÃ£ lÆ°u {len(embedded_chunks)} chunks vÃ o MongoDB")

    os.makedirs("output_chunks", exist_ok=True)
    out_path = os.path.join("output_chunks", f"chunks_{timestamp}.json")
    cleaned = clean_objectid(embedded_chunks)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(cleaned, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“¦ ÄÃ£ lÆ°u vÃ o: {out_path}")
    return {
        "embedded_chunks": embedded_chunks,
        "output_path": out_path,
    }


class FilterChunks:
    def __init__(self):
        self.engine = SemanticChunkFilter()

    def __call__(self, state: FlowState):
        uploaded_chunks = state.get("uploaded_chunks", [])
        all_chunks = state.get("all_chunks", [])
        subtopics = state.get("subtopics", [])

        if uploaded_chunks:
            chunks = uploaded_chunks
            print(f"\nğŸ“‚ Lá»c {len(chunks)} uploaded chunks theo {len(subtopics)} subtopics...")
        else:
            chunks = all_chunks
            print(f"\nğŸŒ Lá»c {len(chunks)} (db + search) chunks theo {len(subtopics)} subtopics...")

        if not chunks or not subtopics:
            print("\nâš ï¸ KhÃ´ng cÃ³ chunks hoáº·c subtopics Ä‘á»ƒ lá»c")
            return {"filtered_chunks": chunks}

        if len(chunks) <= 5:
            print(f"\nâ„¹ï¸ Chá»‰ cÃ³ {len(chunks)} chunks â€” bá» qua bÆ°á»›c lá»c semantic.")
            return {"filtered_chunks": chunks}

        filtered = self.engine.filter(chunks, subtopics)
        print(f"âœ… ÄÃ£ lá»c cÃ²n {len(filtered)} chunks liÃªn quan")
        return {"filtered_chunks": filtered}


class GenerateLessonPlan:
    def __init__(self, llm: GPTClient):
        self.pipeline = LessonPlanPipeline(llm)

    def __call__(self, state: FlowState):
        # Kiá»ƒm tra cÃ³ cáº§n táº¡o lesson plan khÃ´ng
        if not should_generate_lesson_plan(state):
            print("â­ï¸ Skip táº¡o Lesson Plan (user khÃ´ng tick)")
            return {"lesson_plan": {}, "__plan_done__": True}
        print("\nğŸ“ Báº¯t Ä‘áº§u táº¡o káº¿ hoáº¡ch bÃ i giáº£ng...")
        prompt = state.get("user_prompt", "")
        filtered_chunks = state.get("filtered_chunks", [])
        if not prompt:
            return {"lesson_plan": {"error": "KhÃ´ng cÃ³ prompt Ä‘á»ƒ táº¡o bÃ i giáº£ng"}, "__plan_done__": True}
        lesson_plan = self.pipeline.create_full_lesson_plan(prompt, filtered_chunks)
        print("âœ… HoÃ n thÃ nh táº¡o káº¿ hoáº¡ch bÃ i giáº£ng!")
        if "output_path" in lesson_plan:
            print(f"ğŸ“ ÄÃ£ lÆ°u táº¡i: {lesson_plan['output_path']}")
        return {"lesson_plan": lesson_plan, "__plan_done__": True}


class GenerateQuiz:
    def __init__(self, llm: GPTClient):
        self.pipeline = QuizPipeline(llm)

    def __call__(self, state: FlowState):
        # Kiá»ƒm tra cÃ³ cáº§n táº¡o quiz khÃ´ng
        if not should_generate_quiz(state):
            print("â­ï¸ Skip táº¡o Quiz (user khÃ´ng tick)")
            return {"quiz": {}, "__quiz_done__": True}
        
        print("\nğŸ“ Báº¯t Ä‘áº§u táº¡o Quiz...")
        form = state.get("form_data", {}) or {}
        mode = form.get("quiz_source", "material")  # "material" | "plan"
        config = form.get("quiz_config", {})
        prompt = state.get("user_prompt", "")
        filtered_chunks = state.get("filtered_chunks", [])
        lesson_plan = state.get("lesson_plan", {})

        if mode == "plan" and not lesson_plan:
            print("âš ï¸ KhÃ´ng cÃ³ lesson plan Ä‘á»ƒ sinh quiz. Fallback sang material.")
            mode = "material"

        # --- Gá»i pipeline vá»›i fallback ---
        if mode == "plan":
            if hasattr(self.pipeline, "from_lesson_plan"):
                quiz = self.pipeline.from_lesson_plan(lesson_plan, prompt, config)
            else:
                # Fallback: fuse lesson plan vÃ o prompt rá»“i táº¡o quiz tá»« chunks
                plan_md = (
                    lesson_plan.get("complete_markdown")
                    or lesson_plan.get("markdown", "")
                    or json.dumps(lesson_plan, ensure_ascii=False)
                )
                augmented_prompt = f"{prompt}\n\n[LESSON_PLAN]\n{plan_md}"
                quiz = self.pipeline.create_full_quiz(augmented_prompt, filtered_chunks)
        else:
            if hasattr(self.pipeline, "from_materials"):
                quiz = self.pipeline.from_materials(prompt, filtered_chunks, config)
            else:
                quiz = self.pipeline.create_full_quiz(prompt, filtered_chunks)

        print("âœ… HoÃ n thÃ nh táº¡o Quiz!")
        if isinstance(quiz, dict) and "output_path" in quiz:
            print(f"ğŸ“ ÄÃ£ lÆ°u quiz táº¡i: {quiz['output_path']}")

        return {"quiz": quiz, "__quiz_done__": True}


# ========================= Routing funcs =========================
def should_call_agent(state: FlowState):
    """Sau process_file: náº¿u khÃ´ng cÃ³ file => agent_retrieval, ngÆ°á»£c láº¡i embed uploaded."""
    return "agent_retrieval" if state.get("__skip__") else "embed_store_uploaded"


def first_after_filter(state: FlowState):
    """Sau filter_chunks: quyáº¿t Ä‘á»‹nh sinh plan hay quiz trÆ°á»›c dá»±a trÃªn checkbox."""
    form = state.get("form_data", {}) or {}
    
    # Láº¥y content_types tá»« form (tá»« checkbox)
    content_types = form.get("content_types", [])
    
    # DEBUG: In ra Ä‘á»ƒ kiá»ƒm tra
    print(f"ğŸ” [DEBUG] form keys: {list(form.keys())}")
    print(f"ğŸ” [DEBUG] content_types value: {content_types}")
    print(f"ğŸ” [DEBUG] content_types type: {type(content_types)}")
    
    # Ensure content_types is a list
    if not isinstance(content_types, list):
        if isinstance(content_types, str):
            content_types = [content_types]
        else:
            content_types = []
    
    # Convert thÃ nh set Ä‘á»ƒ dá»… so sÃ¡nh
    if isinstance(content_types, list):
        outputs = set(content_types)
    else:
        print(f"âš ï¸ [WARNING] content_types khÃ´ng pháº£i list: {content_types}")
        outputs = set()
    
    print(f"ğŸ¯ [first_after_filter] User chá»n: {outputs}")
    
    # Convert thÃ nh set Ä‘á»ƒ dá»… so sÃ¡nh
    outputs = set(content_types)
    print(f"ğŸ¯ [first_after_filter] User chá»n: {outputs}")
    
    # Case 1: Chá»‰ táº¡o lesson plan
    if outputs == {"lesson_plan"}:
        print("âœ… Chá»‰ táº¡o Káº¿ hoáº¡ch giáº£ng dáº¡y")
        return "generate_lesson_plan"
    
    # Case 2: Chá»‰ táº¡o quiz  
    if outputs == {"quiz"}:
        print("âœ… Chá»‰ táº¡o Quiz")
        return "generate_quiz"
    
    # Case 3: Táº¡o cáº£ hai - luÃ´n táº¡o plan trÆ°á»›c Ä‘á»ƒ quiz cÃ³ thá»ƒ sá»­ dá»¥ng
    if "lesson_plan" in outputs and "quiz" in outputs:
        print("âœ… Táº¡o cáº£ hai: Plan trÆ°á»›c, Quiz sau")
        return "generate_lesson_plan"
    
    # Fallback: máº·c Ä‘á»‹nh táº¡o lesson plan náº¿u cÃ³ lesson_plan
    if "lesson_plan" in outputs:
        print(f"âš ï¸ Fallback táº¡o lesson plan tá»« {outputs}")
        return "generate_lesson_plan"
    elif "quiz" in outputs:
        print(f"âš ï¸ Fallback táº¡o quiz tá»« {outputs}")
        return "generate_quiz"
    else:
        print(f"âŒ KhÃ´ng cÃ³ lá»±a chá»n há»£p lá»‡ tá»« {outputs}, máº·c Ä‘á»‹nh táº¡o lesson plan")
        return "generate_lesson_plan"


def route_after_plan(state: FlowState):
    """Sau generate_lesson_plan: kiá»ƒm tra cÃ²n cáº§n quiz khÃ´ng."""
    form = state.get("form_data", {}) or {}
    content_types = form.get("content_types", [])
    
    # Ensure it's a list
    if not isinstance(content_types, list):
        if isinstance(content_types, str):
            content_types = [content_types]
        else:
            content_types = []
    
    outputs = set(content_types)
    print(f"ğŸ”„ [route_after_plan] ÄÃ£ hoÃ n thÃ nh Plan. User chá»n: {outputs}")
    
    # Náº¿u user cÃ³ tick quiz vÃ  chÆ°a lÃ m quiz
    if "quiz" in outputs and not state.get("__quiz_done__", False):
        print("â¡ï¸ Tiáº¿p tá»¥c táº¡o Quiz")
        return "generate_quiz"
    
    print("ğŸ HoÃ n thÃ nh - chá»‰ cáº§n Plan")
    return "END"


def route_after_quiz(state: FlowState):
    """Sau generate_quiz: kiá»ƒm tra cÃ²n cáº§n plan khÃ´ng."""
    form = state.get("form_data", {}) or {}
    content_types = form.get("content_types", [])
    
    # Ensure it's a list
    if not isinstance(content_types, list):
        if isinstance(content_types, str):
            content_types = [content_types]
        else:
            content_types = []
    
    outputs = set(content_types)
    print(f"ğŸ”„ [route_after_quiz] ÄÃ£ hoÃ n thÃ nh Quiz. User chá»n: {outputs}")
    
    # Náº¿u user cÃ³ tick plan vÃ  chÆ°a lÃ m plan  
    if "lesson_plan" in outputs and not state.get("__plan_done__", False):
        print("â¡ï¸ Tiáº¿p tá»¥c táº¡o Lesson Plan")
        return "generate_lesson_plan"
    
    print("ğŸ HoÃ n thÃ nh - chá»‰ cáº§n Quiz")
    return "END"

##Skip Logic
def should_generate_lesson_plan(state: FlowState):
    """Kiá»ƒm tra cÃ³ cáº§n táº¡o lesson plan khÃ´ng."""
    form = state.get("form_data", {}) or {}
    content_types = form.get("content_types", [])
    
    if not isinstance(content_types, list):
        if isinstance(content_types, str):
            content_types = [content_types]
        else:
            content_types = []
    
    should_generate = "lesson_plan" in content_types
    print(f"ğŸ¤” [should_generate_lesson_plan] {should_generate} (from {content_types})")
    return should_generate

def should_generate_quiz(state: FlowState):
    """Kiá»ƒm tra cÃ³ cáº§n táº¡o quiz khÃ´ng."""  
    form = state.get("form_data", {}) or {}
    content_types = form.get("content_types", [])
    
    if not isinstance(content_types, list):
        if isinstance(content_types, str):
            content_types = [content_types]
        else:
            content_types = []
    
    should_generate = "quiz" in content_types
    print(f"ğŸ¤” [should_generate_quiz] {should_generate} (from {content_types})")
    return should_generate

# ========================= LLMs =========================
llm = GPTClient(
    api_key=os.environ.get("AZURE_API_KEY"),
    endpoint=os.environ.get("AZURE_ENDPOINT"),
    model=os.environ.get("AZURE_MODEL"),
    api_version=os.environ.get("AZURE_API_VERSION")
)

# CÃ³ thá»ƒ dÃ¹ng Gemini náº¿u muá»‘n
gemini_llm = GeminiClient(
    api_key=os.environ.get("GEMINI_API_KEY"),
    model="gemini-2.5-flash"
)


# ========================= Build Graph =========================
builder = StateGraph(FlowState)

builder.add_node("generate_prompt", PromptGenerator(llm))
builder.add_node("generate_subtopics", SubtopicGenerator(llm))
builder.add_node("process_file", FileProcessor())
builder.add_node("agent_retrieval", AgentBasedRetrieval())
builder.add_node("embed_store_uploaded", EmbedAndStoreUploaded())
builder.add_node("embed_store_searched", EmbedAndStoreSearched())
builder.add_node("filter_chunks", FilterChunks())
builder.add_node("generate_lesson_plan", GenerateLessonPlan(llm))
builder.add_node("generate_quiz", GenerateQuiz(llm))

builder.set_entry_point("generate_prompt")

# Linear edges
builder.add_edge("generate_prompt", "generate_subtopics")
builder.add_edge("generate_subtopics", "process_file")

# Branch after process_file
builder.add_conditional_edges(
    "process_file", should_call_agent,
    {"embed_store_uploaded": "embed_store_uploaded", "agent_retrieval": "agent_retrieval"}
)

# Continue embedding / retrieval to filter
builder.add_edge("agent_retrieval", "embed_store_searched")
builder.add_edge("embed_store_uploaded", "filter_chunks")
builder.add_edge("embed_store_searched", "filter_chunks")

# Decide which product first
builder.add_conditional_edges(
    "filter_chunks", first_after_filter,
    {"generate_lesson_plan": "generate_lesson_plan", "generate_quiz": "generate_quiz"}
)

# After plan -> maybe quiz / end
builder.add_conditional_edges(
    "generate_lesson_plan", route_after_plan,
    {"generate_quiz": "generate_quiz", "END": END}
)

# After quiz -> maybe plan / end
builder.add_conditional_edges(
    "generate_quiz", route_after_quiz,
    {"generate_lesson_plan": "generate_lesson_plan", "END": END}
)

# ========================= Compile & Run =========================
graph = builder.compile()

def run_flow(form_data: dict):
    result = graph.invoke({"form_data": form_data})
    return result


# ========================= Viz =========================
try:
    graph.get_graph().draw_png("langgraph_flow.png")
    print("ÄÃ£ táº¡o sÆ¡ Ä‘á»“ flow táº¡i: langgraph_flow.png")
except Exception as e:
    print(f"KhÃ´ng thá»ƒ táº¡o sÆ¡ Ä‘á»“ trá»±c tiáº¿p (lá»—i: {e}). Äáº£m báº£o Ä‘Ã£ cÃ i 'pygraphviz'/'pydot' vÃ  'graphviz'.")
    print("\nÄang thá»­ xuáº¥t cáº¥u trÃºc Ä‘á»“ thá»‹ sang JSON.")
    try:
        graph_json = graph.get_graph().to_json()
        output_json_path = "langgraph_flow.json"
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(graph_json, f, ensure_ascii=False, indent=2)
        print(f"âœ… Xuáº¥t cáº¥u trÃºc Ä‘á»“ thá»‹: {output_json_path}")
    except Exception as json_e:
        print(f"KhÃ´ng thá»ƒ xuáº¥t Ä‘á»“ thá»‹ sang JSON: {json_e}")
        try:
            print(graph.get_graph().get_graph().to_string())
        except Exception as dot_e:
            print(f"KhÃ´ng thá»ƒ láº¥y DOT string: {dot_e}")
