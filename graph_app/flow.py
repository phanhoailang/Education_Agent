import sys
import os

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

import os
import json
import datetime
from pathlib import Path
from langgraph.graph import StateGraph, END
from typing import TypedDict
from bson import ObjectId
from dotenv import load_dotenv

from utils.GPTClient import GPTClient
from utils.GeminiClient import GeminiClient
from modules.agents.ChatAgent import ChatAgent
from modules.agents.SubtopicGeneratorAgent import SubtopicGeneratorAgent
from modules.rag_module.SemanticChunkFilter import SemanticChunkFilter
from modules.rag_module.documents_processing.main_processor import EduMateDocumentProcessor
from modules.rag_module.data_chunking.processor import IntelligentVietnameseChunkingProcessor
from modules.rag_module.data_embedding.embedding_processor import VietnameseEmbeddingProcessor
from modules.rag_module.query_db.MongoDBClient import MongoDBClient
from modules.rag_module.DeepRetrieval import OptimizedDeepRetrieval, DeepRetrieval
from modules.rag_module.SemanticChunkFilter import SemanticChunkFilter
from modules.lesson_plan.LessonPlanPipeline import LessonPlanPipeline

load_dotenv()

# ‚úÖ H√†m l·ªçc ObjectId
def clean_objectid(obj):
    if isinstance(obj, dict):
        return {k: clean_objectid(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_objectid(i) for i in obj]
    elif isinstance(obj, ObjectId):
        return str(obj)
    else:
        return obj

# ‚úÖ 1. Khai b√°o tr·∫°ng th√°i
class FlowState(TypedDict):
    form_data: dict
    user_prompt: str
    subtopics: list
    db_chunks: list
    uploaded_chunks: list
    search_chunks: list
    all_chunks: list
    embedded_chunks: list
    filtered_chunks: list  # ‚úÖ TH√äM
    lesson_plan: dict      # ‚úÖ TH√äM
    output_path: str
    __skip__: bool

# ‚úÖ 2. Step: Sinh prompt t·ª´ form
class PromptGenerator:
    def __init__(self, llm: GPTClient):
        self.agent = ChatAgent(llm)

    def __call__(self, state: FlowState):
        result = self.agent.run(mode="generate_prompt", form_data=state["form_data"])
        print("\nüß† Prompt g·ªëc sinh t·ª´ form:")
        print(result)

        return {
            "user_prompt": result
        }
    
# ‚úÖ 3. Step: Sinh c√°c subtopics t·ª´ prompt
class SubtopicGenerator:
    def __init__(self, llm: GPTClient):
        self.agent = SubtopicGeneratorAgent(llm)

    def __call__(self, state: FlowState):
        prompt = state["user_prompt"]
        subtopics = self.agent.run(prompt)

        print(f"\nüìå ƒê√£ sinh {len(subtopics)} subtopics:")
        for i, topic in enumerate(subtopics, 1):
            print(f"   {i}. {topic}")

        return {"subtopics": subtopics}

# ‚úÖ 4. Step: X·ª≠ l√Ω file ng∆∞·ªùi d√πng
class FileProcessor:
    def __init__(self):
        self.document_processor = EduMateDocumentProcessor.create_balanced()
        self.chunking_processor = IntelligentVietnameseChunkingProcessor(
            output_dir="temp_langgraph_chunking",
            min_quality=0.65
        )
        
    def __call__(self, state: FlowState):
        form = state["form_data"]
        files = form.get("files", [])
        if not files:
            print("\nKh√¥ng c√≥ file ƒë√≠nh k√®m. S·∫Ω g·ªçi agent truy v·∫•n sau...")
            return {"__skip__": True, "search_chunks": [], "db_chunks": [], "all_chunks": []}

        all_standardized_chunks = []

        for file_path in files:
            print(f"\nüìÇ ƒêang x·ª≠ l√Ω file: {file_path}")

            try:
                file_path_obj = Path(file_path)
                
                # Step 1: Document processing
                doc_result = self.document_processor.process_file(file_path_obj)
                
                if not doc_result.success:
                    print(f"Document processing failed: {doc_result.error_message}")
                    continue  # B·ªè qua file n√†y v√† chuy·ªÉn sang file ti·∫øp theo

                import tempfile
                with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as temp_file:
                    temp_file.write(doc_result.content)
                    temp_md_path = temp_file.name
                
                # Step 2: Intelligent chunking
                chunking_result = self.chunking_processor.run(
                    Path(temp_md_path),
                    strategy=None,
                    save_json=False,
                    print_report=False
                )
                
                chunks_data = chunking_result['result']['chunking_results']['chunks_data']
                
                # Step 3: Convert to standardized format
                for i, chunk_data in enumerate(chunks_data):
                    all_standardized_chunks.append({
                        "chunk_id": f"{Path(file_path).stem}_chunk_{i+1:02d}",  # Th√™m t√™n file v√†o chunk_id
                        "content": chunk_data["content"],
                        "token_count": chunk_data.get("word_count", 0),
                        "method": chunk_data.get("chunking_strategy", "intelligent"),
                        "source_file": file_path,
                        "retrieved_from": "upload",
                        "char_count": chunk_data.get("char_count", 0),
                        "keywords": chunk_data.get("keywords", []),
                        "coherence_score": chunk_data.get("semantic_coherence_score", 0.0),
                        "completeness_score": chunk_data.get("completeness_score", 0.0),
                        "language_confidence": chunk_data.get("language_confidence", 0.0)
                    })
                
                print(f"‚úÖ Processed {file_path}: {len(chunks_data)} chunks created")
                
                # Cleanup temp file
                try:
                    import os
                    os.unlink(temp_md_path)
                except:
                    pass
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to process {file_path}: {e}")
                continue  # Ti·∫øp t·ª•c v·ªõi file ti·∫øp theo
        
        if not all_standardized_chunks:
            print("\n‚ùå Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng")
            return {"__skip__": True, "search_chunks": [], "db_chunks": [], "all_chunks": []}
            
        print(f"\nüéâ T·ªïng c·ªông ƒë√£ x·ª≠ l√Ω: {len(all_standardized_chunks)} chunks t·ª´ {len(files)} files")
        
        return {
            "search_chunks": [],
            "db_chunks": [],
            "uploaded_chunks": all_standardized_chunks,
            "__skip__": False,
            "source_files": files,  # Tr·∫£ v·ªÅ danh s√°ch t·∫•t c·∫£ files
            "processing_method": "new_intelligent"
        }

# ‚úÖ 5. N·∫øu kh√¥ng c√≥ file ‚Üí truy v·∫•n DB + search ngo√†i
class AgentBasedRetrieval:
    def __init__(self):
        self.retriever = OptimizedDeepRetrieval()

    def __call__(self, state: FlowState):
        print("üß† [agent_retrieval] ƒê√É ƒê∆Ø·ª¢C G·ªåI")
        prompt = state.get("user_prompt", "")
        subtopics = state.get("subtopics", [])
        if not prompt or not subtopics:
            print("\nThi·∫øu prompt ho·∫∑c subtopics.")
            return {}

        if hasattr(self, 'retriever'):
            chunks = self.retriever.retrieve(prompt, subtopics)
        else:
            chunks = DeepRetrieval(prompt, subtopics)

        db_chunks = [c for c in chunks if c.get("retrieved_from") == "db"]
        search_chunks = [c for c in chunks if c.get("retrieved_from") == "search"]
        all_chunks = db_chunks + search_chunks

        print(f"\nüì¶ T·ªïng c·ªông: {len(all_chunks)} chunks (DB: {len(db_chunks)}, Search: {len(search_chunks)})")

        return {
            "db_chunks": db_chunks,
            "search_chunks": search_chunks,
            "all_chunks": all_chunks
        }

# ‚úÖ 6. Embedding + l∆∞u CSDL (ch·ªâ cho chunks search ho·∫∑c upload)
class EmbedAndStoreUploaded:
    def __call__(self, state: FlowState):
        chunks = state.get("uploaded_chunks", [])
        if not chunks:
            print("\n Kh√¥ng c√≥ chunks ƒë·ªÉ embedding (uploaded).")
            return {}

        file_path = state.get("source_file", "upload")
        source_name = os.path.basename(file_path)

        return embed_and_store_chunks(chunks, source_name)


class EmbedAndStoreSearched:
    def __call__(self, state: FlowState):
        chunks = state.get("search_chunks", [])
        if not chunks:
            print("\n‚ùå Kh√¥ng c√≥ chunks ƒë·ªÉ embedding (searched).")
            return {}

        return embed_and_store_chunks(chunks, "web_search")

# ‚úÖ H·ªó tr·ª£: chia s·∫ª logic embedding & l∆∞u
def embed_and_store_chunks(chunks, source_files):
    print("\nüìÑ Chunks:")
    print(json.dumps(chunks[:3], ensure_ascii=False, indent=2))
    if len(chunks) > 3:
        print(f"... (c√≤n {len(chunks) - 3} chunks n·ªØa)\n")

    chunks_with_metadata = []
    for chunk in chunks:
        if "metadata" not in chunk:
            chunk["metadata"] = {}
        
        # Th√™m source file info v√†o metadata
        chunk["metadata"]["source_file"] = chunk.get("source_file", "unknown")
        chunk["metadata"]["all_source_files"] = source_files if isinstance(source_files, list) else [source_files]
        chunk["metadata"]["collection"] = "lectures"
        
        chunks_with_metadata.append(chunk)

    os.makedirs("temp_embedding", exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    temp_path = os.path.join("temp_embedding", f"temp_chunks_{timestamp}.json")

    with open(temp_path, "w", encoding="utf-8") as f:
        json.dump(chunks_with_metadata, f, ensure_ascii=False, indent=2)

    embedder = VietnameseEmbeddingProcessor()
    result = embedder.run(temp_path, save_results=False)
    embedded_chunks = result["chunks"]

    try:
        os.remove(temp_path)
    except:
        pass

    db = MongoDBClient()
    db.insert_many("lectures", embedded_chunks)
    print(f"\n‚úÖ ƒê√£ l∆∞u {len(embedded_chunks)} chunks v√†o MongoDB")

    os.makedirs("output_chunks", exist_ok=True)
    os.makedirs("output_chunks", exist_ok=True)
    out_path = os.path.join("output_chunks", f"chunks_{timestamp}.json")

    cleaned = clean_objectid(embedded_chunks)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(cleaned, f, ensure_ascii=False, indent=2)

    print(f"\nüì¶ ƒê√£ l∆∞u v√†o: {out_path}")
    return {
        "embedded_chunks": embedded_chunks, 
        "output_path": out_path,
    }

# ‚úÖ 7. L·ªçc semantic
class FilterChunks:
    def __init__(self):
        self.engine = SemanticChunkFilter()
        
    def __call__(self, state: FlowState):
        uploaded_chunks = state.get("uploaded_chunks", [])
        all_chunks = state.get("all_chunks", [])
        subtopics = state.get("subtopics", [])

        # ∆Øu ti√™n l·ªçc uploaded n·∫øu c√≥
        if uploaded_chunks:
            chunks = uploaded_chunks
            print(f"\nüìÇ L·ªçc {len(chunks)} uploaded chunks theo {len(subtopics)} subtopics...")
        else:
            chunks = all_chunks
            print(f"\nüåê L·ªçc {len(chunks)} (db + search) chunks theo {len(subtopics)} subtopics...")

        if not chunks or not subtopics:
            print("\n‚ö†Ô∏è Kh√¥ng c√≥ chunks ho·∫∑c subtopics ƒë·ªÉ l·ªçc")
            return {"filtered_chunks": chunks}

        if len(chunks) <= 5:
            print(f"\n‚ÑπÔ∏è Ch·ªâ c√≥ {len(chunks)} chunks ‚Äî b·ªè qua b∆∞·ªõc l·ªçc semantic.")
            return {"filtered_chunks": chunks}

        engine = SemanticChunkFilter()
        filtered = engine.filter(chunks, subtopics)

        print(f"‚úÖ ƒê√£ l·ªçc c√≤n {len(filtered)} chunks li√™n quan")
        return {"filtered_chunks": filtered}
    
# ‚úÖ 7. Sinh s∆∞·ªùn b√†i gi·∫£ng
class GenerateLessonPlan:
    def __init__(self, llm: GPTClient):
    # def __init__(self, llm: GeminiClient):
        self.pipeline = LessonPlanPipeline(llm)
        
    def __call__(self, state: FlowState):
        print("\nüéì B·∫Øt ƒë·∫ßu t·∫°o k·∫ø ho·∫°ch b√†i gi·∫£ng...")
        
        prompt = state.get("user_prompt", "")
        filtered_chunks = state.get("filtered_chunks", [])
        
        if not prompt:
            return {"lesson_plan": {"error": "Kh√¥ng c√≥ prompt ƒë·ªÉ t·∫°o b√†i gi·∫£ng"}}
        
        # T·∫°o k·∫ø ho·∫°ch b√†i gi·∫£ng ƒë·∫ßy ƒë·ªß
        lesson_plan = self.pipeline.create_full_lesson_plan(prompt, filtered_chunks)
        
        print(f"‚úÖ Ho√†n th√†nh t·∫°o k·∫ø ho·∫°ch b√†i gi·∫£ng!")
        if "output_path" in lesson_plan:
            print(f"üìÅ ƒê√£ l∆∞u t·∫°i: {lesson_plan['output_path']}")
        
        return {"lesson_plan": lesson_plan}

# ‚úÖ ƒêi·ªÅu ki·ªán r·∫Ω nh√°nh

def should_call_agent(state: FlowState):
    return "agent_retrieval" if state.get("__skip__") else "embed_store_uploaded"

llm = GPTClient(
    api_key=os.environ.get("AZURE_API_KEY"),
    endpoint=os.environ.get("AZURE_ENDPOINT"),
    model=os.environ.get("AZURE_MODEL"),
    api_version=os.environ.get("AZURE_API_VERSION")
)

gemini_llm = GeminiClient(
    api_key=os.environ.get("GEMINI_API_KEY"),
    model="gemini-2.5-flash"
)

# ‚úÖ Build LangGraph
builder = StateGraph(FlowState)
builder.add_node("generate_prompt", PromptGenerator(llm))
builder.add_node("generate_subtopics", SubtopicGenerator(llm))
builder.add_node("process_file", FileProcessor())
builder.add_node("agent_retrieval", AgentBasedRetrieval())
builder.add_node("embed_store_uploaded", EmbedAndStoreUploaded())
builder.add_node("embed_store_searched", EmbedAndStoreSearched())
builder.add_node("filter_chunks", FilterChunks())
builder.add_node("generate_lesson_plan", GenerateLessonPlan(llm))
# builder.add_node("generate_lesson_plan", GenerateLessonPlan(gemini_llm))

builder.set_entry_point("generate_prompt")
builder.add_edge("generate_prompt", "generate_subtopics")
builder.add_edge("generate_subtopics", "process_file")
builder.add_conditional_edges("process_file", should_call_agent, {
    "embed_store_uploaded": "embed_store_uploaded",
    "agent_retrieval": "agent_retrieval"
})
builder.add_edge("agent_retrieval", "embed_store_searched")
builder.add_edge("embed_store_uploaded", "filter_chunks")
builder.add_edge("embed_store_searched", "filter_chunks")
builder.add_edge("filter_chunks", "generate_lesson_plan")  # ‚úÖ S·ª¨A
builder.add_edge("generate_lesson_plan", END)              # ‚úÖ TH√äM


graph = builder.compile()

def run_flow(form_data: dict):
    result = graph.invoke({"form_data": form_data})
    return result


try:
    graph.get_graph().draw_png("langgraph_flow.png")
    print("ƒê√£ t·∫°o s∆° ƒë·ªì flow t·∫°i: langgraph_flow.png")
except Exception as e:
    print(f"Kh√¥ng th·ªÉ t·∫°o s∆° ƒë·ªì tr·ª±c ti·∫øp (l·ªói: {e}). ƒê·∫£m b·∫£o b·∫°n ƒë√£ c√†i ƒë·∫∑t 'pygraphviz' ho·∫∑c 'pydot' v√† 'graphviz'.")
    print("\nƒêang th·ª≠ xu·∫•t c·∫•u tr√∫c ƒë·ªì th·ªã sang ƒë·ªãnh d·∫°ng JSON ƒë·ªÉ b·∫°n c√≥ th·ªÉ tr·ª±c quan h√≥a th·ªß c√¥ng.")
    try:
        graph_json = graph.get_graph().to_json()
        output_json_path = "langgraph_flow.json"
        with open(output_json_path, "w", encoding="utf-8") as f:
            json.dump(graph_json, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ ƒê√£ xu·∫•t c·∫•u tr√∫c ƒë·ªì th·ªã th√†nh c√¥ng sang: {output_json_path}")
        print("   B·∫°n c√≥ th·ªÉ d√πng c√°c c√¥ng c·ª• tr·ª±c tuy·∫øn nh∆∞ 'Mermaid Live Editor' (https://mermaid.live/)")
        print("   ho·∫∑c 'GraphvizOnline' (https://dreampuf.github.io/GraphvizOnline/) ƒë·ªÉ d√°n n·ªôi dung JSON v√† xem.")
    except Exception as json_e:
        print(f"Kh√¥ng th·ªÉ xu·∫•t ƒë·ªì th·ªã sang JSON: {json_e}")
        print("\nD∆∞·ªõi ƒë√¢y l√† ƒë·ªãnh d·∫°ng DOT c·ªßa ƒë·ªì th·ªã (c√≥ th·ªÉ kh√¥ng ƒë·∫ßy ƒë·ªß n·∫øu c√≥ l·ªói):")
        # Fallback cu·ªëi c√πng l√† in ra DOT n·∫øu m·ªçi th·ª© kh√°c th·∫•t b·∫°i
        try:
            print(graph.get_graph().get_graph().to_string())
        except Exception as dot_e:
            print(f"Kh√¥ng th·ªÉ l·∫•y chu·ªói DOT: {dot_e}")