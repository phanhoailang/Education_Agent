import statistics
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import logging

from .chunk_metadata import ChunkMetadata

@dataclass
class ChunkingEvaluation:
    """K·∫øt qu·∫£ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng chunking."""
    
    # Metrics c∆° b·∫£n
    total_chunks: int
    avg_chunk_size: float
    min_chunk_size: int
    max_chunk_size: int
    std_chunk_size: float
    
    # Metrics ch·∫•t l∆∞·ª£ng
    avg_coherence_score: float
    avg_completeness_score: float
    avg_language_confidence: float
    
    # Metrics c·∫•u tr√∫c
    complete_sentences_ratio: float
    paragraph_preservation_ratio: float
    overlap_efficiency: float
    
    # Metrics ƒë·∫∑c tr∆∞ng ti·∫øng Vi·ªát
    vietnamese_chars_ratio: float
    avg_words_per_sentence: float
    named_entities_count: int
    pos_diversity: float
    
    # T·ªïng ƒëi·ªÉm
    overall_quality_score: float
    
    # Th√¥ng tin chi ti·∫øt
    strategy_name: str
    processing_time: float
    recommendations: List[str]

class ChunkQualityEvaluator:
    """ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng chunking cho vƒÉn b·∫£n ti·∫øng Vi·ªát."""
    
    def __init__(self, 
                 min_acceptable_coherence: float = 0.6,
                 min_acceptable_completeness: float = 0.7,
                 target_chunk_size: int = 1000,
                 size_tolerance: float = 0.3):
        """
        Args:
            min_acceptable_coherence: ƒêi·ªÉm coherence t·ªëi thi·ªÉu
            min_acceptable_completeness: ƒêi·ªÉm completeness t·ªëi thi·ªÉu
            target_chunk_size: K√≠ch th∆∞·ªõc chunk m·ª•c ti√™u
            size_tolerance: ƒê·ªô ch·∫•p nh·∫≠n sai l·ªách k√≠ch th∆∞·ªõc (30% = 0.3)
        """
        self.min_acceptable_coherence = min_acceptable_coherence
        self.min_acceptable_completeness = min_acceptable_completeness
        self.target_chunk_size = target_chunk_size
        self.size_tolerance = size_tolerance
        
        self.logger = logging.getLogger(__name__)
    
    def evaluate_chunks(self, 
                       chunks: List[ChunkMetadata],
                       original_text: Optional[str] = None) -> ChunkingEvaluation:
        """
        ƒê√°nh gi√° to√†n di·ªán ch·∫•t l∆∞·ª£ng chunking.
        
        Args:
            chunks: Danh s√°ch chunks ƒë·ªÉ ƒë√°nh gi√°
            original_text: VƒÉn b·∫£n g·ªëc (ƒë·ªÉ t√≠nh preservation metrics)
            
        Returns:
            K·∫øt qu·∫£ ƒë√°nh gi√° chi ti·∫øt
        """
        if not chunks:
            return self._empty_evaluation()
        
        # T√≠nh metrics c∆° b·∫£n
        basic_metrics = self._calculate_basic_metrics(chunks)
        
        # T√≠nh metrics ch·∫•t l∆∞·ª£ng
        quality_metrics = self._calculate_quality_metrics(chunks)
        
        # T√≠nh metrics c·∫•u tr√∫c
        structure_metrics = self._calculate_structure_metrics(chunks, original_text)
        
        # T√≠nh metrics ti·∫øng Vi·ªát
        vietnamese_metrics = self._calculate_vietnamese_metrics(chunks)
        
        # T√≠nh t·ªïng ƒëi·ªÉm
        overall_score = self._calculate_overall_score(
            basic_metrics, quality_metrics, structure_metrics, vietnamese_metrics
        )
        
        # T·∫°o recommendations
        recommendations = self._generate_recommendations(
            chunks, basic_metrics, quality_metrics, structure_metrics
        )
        
        # L·∫•y th√¥ng tin strategy v√† processing time
        strategy_name = chunks[0].chunking_strategy if chunks else "unknown"
        avg_processing_time = statistics.mean([
            chunk.processing_time or 0.0 for chunk in chunks
        ])
        
        return ChunkingEvaluation(
            # Basic metrics
            total_chunks=basic_metrics['total_chunks'],
            avg_chunk_size=basic_metrics['avg_size'],
            min_chunk_size=basic_metrics['min_size'],
            max_chunk_size=basic_metrics['max_size'],
            std_chunk_size=basic_metrics['std_size'],
            
            # Quality metrics
            avg_coherence_score=quality_metrics['avg_coherence'],
            avg_completeness_score=quality_metrics['avg_completeness'],
            avg_language_confidence=quality_metrics['avg_language_confidence'],
            
            # Structure metrics
            complete_sentences_ratio=structure_metrics['complete_sentences_ratio'],
            paragraph_preservation_ratio=structure_metrics['paragraph_preservation'],
            overlap_efficiency=structure_metrics['overlap_efficiency'],
            
            # Vietnamese metrics
            vietnamese_chars_ratio=vietnamese_metrics['vietnamese_chars_ratio'],
            avg_words_per_sentence=vietnamese_metrics['avg_words_per_sentence'],
            named_entities_count=vietnamese_metrics['named_entities_count'],
            pos_diversity=vietnamese_metrics['pos_diversity'],
            
            # Overall
            overall_quality_score=overall_score,
            strategy_name=strategy_name,
            processing_time=avg_processing_time,
            recommendations=recommendations
        )
    
    def _calculate_basic_metrics(self, chunks: List[ChunkMetadata]) -> Dict[str, Any]:
        """T√≠nh c√°c metrics c∆° b·∫£n."""
        sizes = [chunk.char_count for chunk in chunks]
        
        return {
            'total_chunks': len(chunks),
            'avg_size': statistics.mean(sizes),
            'min_size': min(sizes),
            'max_size': max(sizes),
            'std_size': statistics.stdev(sizes) if len(sizes) > 1 else 0.0
        }
    
    def _calculate_quality_metrics(self, chunks: List[ChunkMetadata]) -> Dict[str, Any]:
        """T√≠nh c√°c metrics ch·∫•t l∆∞·ª£ng."""
        coherence_scores = [
            chunk.semantic_coherence_score or 0.0 for chunk in chunks
        ]
        completeness_scores = [
            chunk.completeness_score or 0.0 for chunk in chunks
        ]
        language_confidences = [
            chunk.language_confidence for chunk in chunks
        ]
        
        return {
            'avg_coherence': statistics.mean(coherence_scores),
            'avg_completeness': statistics.mean(completeness_scores),
            'avg_language_confidence': statistics.mean(language_confidences)
        }
    
    def _calculate_structure_metrics(self, 
                                   chunks: List[ChunkMetadata],
                                   original_text: Optional[str]) -> Dict[str, Any]:
        """T√≠nh c√°c metrics v·ªÅ c·∫•u tr√∫c."""
        
        # T·ª∑ l·ªá chunks k·∫øt th√∫c b·∫±ng c√¢u ho√†n ch·ªânh
        complete_sentences = sum(
            1 for chunk in chunks 
            if chunk.content.strip().endswith(('.', '!', '?'))
        )
        complete_sentences_ratio = complete_sentences / len(chunks)
        
        # Paragraph preservation (n·∫øu c√≥ original text)
        paragraph_preservation = 0.5  # Default
        if original_text:
            original_paragraphs = len([p for p in original_text.split('\n\n') if p.strip()])
            chunk_paragraphs = sum(
                chunk.vietnamese_features.get('paragraph_count', 0) 
                for chunk in chunks
            )
            if original_paragraphs > 0:
                paragraph_preservation = min(1.0, chunk_paragraphs / original_paragraphs)
        
        # Overlap efficiency
        overlaps = [
            chunk.chunk_overlap_prev + chunk.chunk_overlap_next 
            for chunk in chunks
        ]
        avg_overlap = statistics.mean(overlaps) if overlaps else 0
        total_content_length = sum(chunk.char_count for chunk in chunks)
        overlap_efficiency = 1.0 - (avg_overlap / total_content_length) if total_content_length > 0 else 1.0
        
        return {
            'complete_sentences_ratio': complete_sentences_ratio,
            'paragraph_preservation': paragraph_preservation,
            'overlap_efficiency': max(0.0, overlap_efficiency)
        }
    
    def _calculate_vietnamese_metrics(self, chunks: List[ChunkMetadata]) -> Dict[str, Any]:
        """T√≠nh c√°c metrics ƒë·∫∑c tr∆∞ng ti·∫øng Vi·ªát."""
        
        # T·ª∑ l·ªá k√Ω t·ª± ti·∫øng Vi·ªát
        vietnamese_ratios = []
        for chunk in chunks:
            features = chunk.vietnamese_features
            if 'vietnamese_chars_ratio' in features:
                vietnamese_ratios.append(features['vietnamese_chars_ratio'])
        
        vietnamese_chars_ratio = statistics.mean(vietnamese_ratios) if vietnamese_ratios else 0.0
        
        # Trung b√¨nh s·ªë t·ª´ m·ªói c√¢u
        words_per_sentence = []
        for chunk in chunks:
            features = chunk.vietnamese_features
            if 'avg_words_per_sentence' in features:
                words_per_sentence.append(features['avg_words_per_sentence'])
        
        avg_words_per_sentence = statistics.mean(words_per_sentence) if words_per_sentence else 0.0
        
        # Kh√¥ng s·ª≠ d·ª•ng named entities n·ªØa
        total_entities = 0  # B·ªè named entities
        
        # ƒêa d·∫°ng POS tags
        all_pos_tags = set()
        for chunk in chunks:
            all_pos_tags.update(chunk.pos_tags)
        
        pos_diversity = len(all_pos_tags) / 20.0  # Normalize (gi·∫£ s·ª≠ max 20 POS tags)
        pos_diversity = min(1.0, pos_diversity)
        
        return {
            'vietnamese_chars_ratio': vietnamese_chars_ratio,
            'avg_words_per_sentence': avg_words_per_sentence,
            'named_entities_count': total_entities,  # Lu√¥n = 0
            'pos_diversity': pos_diversity
        }
    
    def _calculate_overall_score(self, 
                               basic_metrics: Dict[str, Any],
                               quality_metrics: Dict[str, Any],
                               structure_metrics: Dict[str, Any],
                               vietnamese_metrics: Dict[str, Any]) -> float:
        """T√≠nh t·ªïng ƒëi·ªÉm ch·∫•t l∆∞·ª£ng."""
        
        # Score cho k√≠ch th∆∞·ªõc chunks (0.0 - 1.0)
        avg_size = basic_metrics['avg_size']
        size_diff = abs(avg_size - self.target_chunk_size) / self.target_chunk_size
        size_score = max(0.0, 1.0 - size_diff / self.size_tolerance)
        
        # Score cho consistency k√≠ch th∆∞·ªõc
        std_size = basic_metrics['std_size']
        consistency_score = max(0.0, 1.0 - std_size / avg_size) if avg_size > 0 else 0.0
        
        # K·∫øt h·ª£p c√°c scores v·ªõi tr·ªçng s·ªë
        overall_score = (
            quality_metrics['avg_coherence'] * 0.25 +           # 25%
            quality_metrics['avg_completeness'] * 0.20 +        # 20%
            structure_metrics['complete_sentences_ratio'] * 0.15 + # 15%
            vietnamese_metrics['vietnamese_chars_ratio'] * 0.10 +   # 10%
            quality_metrics['avg_language_confidence'] * 0.10 +     # 10%
            size_score * 0.10 +                                     # 10%
            consistency_score * 0.05 +                              # 5%
            structure_metrics['overlap_efficiency'] * 0.05          # 5%
        )
        
        return min(1.0, max(0.0, overall_score))
    
    def _generate_recommendations(self, 
                                chunks: List[ChunkMetadata],
                                basic_metrics: Dict[str, Any],
                                quality_metrics: Dict[str, Any],
                                structure_metrics: Dict[str, Any]) -> List[str]:
        """T·∫°o recommendations ƒë·ªÉ c·∫£i thi·ªán chunking."""
        recommendations = []
        
        # Ki·ªÉm tra coherence
        if quality_metrics['avg_coherence'] < self.min_acceptable_coherence:
            recommendations.append(
                f"Coherence score th·∫•p ({quality_metrics['avg_coherence']:.2f}). "
                "Th·ª≠ semantic chunking ho·∫∑c sentence-aware strategy."
            )
        
        # Ki·ªÉm tra completeness
        if quality_metrics['avg_completeness'] < self.min_acceptable_completeness:
            recommendations.append(
                f"Completeness score th·∫•p ({quality_metrics['avg_completeness']:.2f}). "
                "TƒÉng overlap ho·∫∑c s·ª≠ d·ª•ng sentence boundary preservation."
            )
        
        # Ki·ªÉm tra k√≠ch th∆∞·ªõc
        avg_size = basic_metrics['avg_size']
        size_diff_ratio = abs(avg_size - self.target_chunk_size) / self.target_chunk_size
        
        if size_diff_ratio > self.size_tolerance:
            if avg_size > self.target_chunk_size:
                recommendations.append(
                    f"Chunks qu√° l·ªõn (trung b√¨nh {avg_size:.0f} chars). "
                    "Gi·∫£m chunk_size ho·∫∑c tƒÉng threshold cho semantic splitting."
                )
            else:
                recommendations.append(
                    f"Chunks qu√° nh·ªè (trung b√¨nh {avg_size:.0f} chars). "
                    "TƒÉng chunk_size ho·∫∑c gi·∫£m threshold cho semantic splitting."
                )
        
        # Ki·ªÉm tra consistency
        std_size = basic_metrics['std_size']
        if std_size > avg_size * 0.5:  # N·∫øu ƒë·ªô l·ªách chu·∫©n > 50% trung b√¨nh
            recommendations.append(
                f"K√≠ch th∆∞·ªõc chunks kh√¥ng ƒë·ªìng ƒë·ªÅu (std: {std_size:.0f}). "
                "Th·ª≠ recursive chunking ho·∫∑c ƒëi·ªÅu ch·ªânh parameters."
            )
        
        # Ki·ªÉm tra sentence completion
        if structure_metrics['complete_sentences_ratio'] < 0.7:
            recommendations.append(
                f"Nhi·ªÅu chunks kh√¥ng k·∫øt th√∫c ho√†n ch·ªânh ({structure_metrics['complete_sentences_ratio']:.1%}). "
                "B·∫≠t sentence boundary preservation."
            )
        
        # Ki·ªÉm tra overlap efficiency
        if structure_metrics['overlap_efficiency'] < 0.8:
            recommendations.append(
                "Overlap kh√¥ng hi·ªáu qu·∫£. C√¢n nh·∫Øc gi·∫£m overlap ratio."
            )
        
        # Ki·ªÉm tra language confidence
        if quality_metrics['avg_language_confidence'] < 0.8:
            recommendations.append(
                f"ƒê·ªô tin c·∫≠y ng√¥n ng·ªØ th·∫•p ({quality_metrics['avg_language_confidence']:.2f}). "
                "Ki·ªÉm tra preprocessing ho·∫∑c s·ª≠ d·ª•ng Vietnamese-specific models."
            )
        
        # N·∫øu kh√¥ng c√≥ v·∫•n ƒë·ªÅ
        if not recommendations:
            recommendations.append("Ch·∫•t l∆∞·ª£ng chunking t·ªët! Kh√¥ng c·∫ßn ƒëi·ªÅu ch·ªânh.")
        
        return recommendations
    
    def _empty_evaluation(self) -> ChunkingEvaluation:
        """Tr·∫£ v·ªÅ evaluation r·ªóng khi kh√¥ng c√≥ chunks."""
        return ChunkingEvaluation(
            total_chunks=0,
            avg_chunk_size=0.0,
            min_chunk_size=0,
            max_chunk_size=0,
            std_chunk_size=0.0,
            avg_coherence_score=0.0,
            avg_completeness_score=0.0,
            avg_language_confidence=0.0,
            complete_sentences_ratio=0.0,
            paragraph_preservation_ratio=0.0,
            overlap_efficiency=0.0,
            vietnamese_chars_ratio=0.0,
            avg_words_per_sentence=0.0,
            named_entities_count=0,
            pos_diversity=0.0,
            overall_quality_score=0.0,
            strategy_name="none",
            processing_time=0.0,
            recommendations=["Kh√¥ng c√≥ chunks ƒë·ªÉ ƒë√°nh gi√°."]
        )
    
    def compare_strategies(self, 
                          evaluations: Dict[str, ChunkingEvaluation]) -> Dict[str, Any]:
        """
        So s√°nh nhi·ªÅu strategies chunking.
        
        Args:
            evaluations: Dict mapping strategy_name -> evaluation
            
        Returns:
            K·∫øt qu·∫£ so s√°nh chi ti·∫øt
        """
        if not evaluations:
            return {"error": "Kh√¥ng c√≥ evaluations ƒë·ªÉ so s√°nh"}
        
        # T√¨m strategy t·ªët nh·∫•t theo t·ª´ng metric
        best_by_metric = {}
        
        metrics = [
            'overall_quality_score',
            'avg_coherence_score', 
            'avg_completeness_score',
            'complete_sentences_ratio',
            'overlap_efficiency',
            'vietnamese_chars_ratio'
        ]
        
        for metric in metrics:
            best_strategy = max(
                evaluations.keys(),
                key=lambda s: getattr(evaluations[s], metric)
            )
            best_value = getattr(evaluations[best_strategy], metric)
            best_by_metric[metric] = {
                'strategy': best_strategy,
                'value': best_value
            }
        
        # T√¨m strategy t·ªët nh·∫•t t·ªïng th·ªÉ
        best_overall = max(
            evaluations.keys(),
            key=lambda s: evaluations[s].overall_quality_score
        )
        
        # T·∫°o ranking
        ranking = sorted(
            evaluations.keys(),
            key=lambda s: evaluations[s].overall_quality_score,
            reverse=True
        )
        
        # T√≠nh to√°n summary statistics
        all_scores = [eval.overall_quality_score for eval in evaluations.values()]
        summary_stats = {
            'avg_score': statistics.mean(all_scores),
            'best_score': max(all_scores),
            'worst_score': min(all_scores),
            'score_range': max(all_scores) - min(all_scores)
        }
        
        return {
            'best_overall_strategy': best_overall,
            'best_overall_score': evaluations[best_overall].overall_quality_score,
            'ranking': ranking,
            'best_by_metric': best_by_metric,
            'summary_statistics': summary_stats,
            'detailed_comparison': self._create_detailed_comparison(evaluations)
        }
    
    def _create_detailed_comparison(self, 
                                  evaluations: Dict[str, ChunkingEvaluation]) -> Dict[str, Dict[str, Any]]:
        """T·∫°o b·∫£ng so s√°nh chi ti·∫øt c√°c strategies."""
        comparison = {}
        
        for strategy, evaluation in evaluations.items():
            comparison[strategy] = {
                'overall_score': evaluation.overall_quality_score,
                'coherence': evaluation.avg_coherence_score,
                'completeness': evaluation.avg_completeness_score,
                'chunk_count': evaluation.total_chunks,
                'avg_size': evaluation.avg_chunk_size,
                'size_consistency': 1.0 - (evaluation.std_chunk_size / evaluation.avg_chunk_size) if evaluation.avg_chunk_size > 0 else 0,
                'sentence_completion': evaluation.complete_sentences_ratio,
                'vietnamese_ratio': evaluation.vietnamese_chars_ratio,
                'processing_time': evaluation.processing_time,
                'recommendation_count': len(evaluation.recommendations)
            }
        
        return comparison
    
    def generate_evaluation_report(self, 
                                 evaluation: ChunkingEvaluation,
                                 detailed: bool = True) -> str:
        """
        T·∫°o b√°o c√°o ƒë√°nh gi√° d·∫°ng text.
        
        Args:
            evaluation: K·∫øt qu·∫£ ƒë√°nh gi√°
            detailed: C√≥ t·∫°o b√°o c√°o chi ti·∫øt kh√¥ng
            
        Returns:
            B√°o c√°o d·∫°ng string
        """
        report = []
        
        # Header
        report.append("=" * 60)
        report.append(f"B√ÅO C√ÅO ƒê√ÅNH GI√Å CHUNKING - {evaluation.strategy_name.upper()}")
        report.append("=" * 60)
        
        # T·ªïng quan
        report.append(f"\nüìä T·ªîNG QUAN:")
        report.append(f"   ‚Ä¢ T·ªïng ƒëi·ªÉm ch·∫•t l∆∞·ª£ng: {evaluation.overall_quality_score:.2f}/1.00")
        report.append(f"   ‚Ä¢ S·ªë chunks: {evaluation.total_chunks}")
        report.append(f"   ‚Ä¢ Th·ªùi gian x·ª≠ l√Ω: {evaluation.processing_time:.2f}s")
        
        # Metrics c∆° b·∫£n
        report.append(f"\nüìè METRICS K√çCH TH∆Ø·ªöC:")
        report.append(f"   ‚Ä¢ K√≠ch th∆∞·ªõc trung b√¨nh: {evaluation.avg_chunk_size:.0f} chars")
        report.append(f"   ‚Ä¢ Kho·∫£ng: {evaluation.min_chunk_size} - {evaluation.max_chunk_size} chars")
        report.append(f"   ‚Ä¢ ƒê·ªô l·ªách chu·∫©n: {evaluation.std_chunk_size:.0f}")
        
        # Metrics ch·∫•t l∆∞·ª£ng
        report.append(f"\n‚ú® METRICS CH·∫§T L∆Ø·ª¢NG:")
        report.append(f"   ‚Ä¢ Coherence: {evaluation.avg_coherence_score:.2f}/1.00")
        report.append(f"   ‚Ä¢ Completeness: {evaluation.avg_completeness_score:.2f}/1.00")
        report.append(f"   ‚Ä¢ ƒê·ªô tin c·∫≠y ng√¥n ng·ªØ: {evaluation.avg_language_confidence:.2f}/1.00")
        
        # Metrics c·∫•u tr√∫c
        report.append(f"\nüèóÔ∏è METRICS C·∫§U TR√öC:")
        report.append(f"   ‚Ä¢ C√¢u ho√†n ch·ªânh: {evaluation.complete_sentences_ratio:.1%}")
        report.append(f"   ‚Ä¢ B·∫£o to√†n ƒëo·∫°n vƒÉn: {evaluation.paragraph_preservation_ratio:.1%}")
        report.append(f"   ‚Ä¢ Hi·ªáu qu·∫£ overlap: {evaluation.overlap_efficiency:.1%}")
        
        # Metrics ti·∫øng Vi·ªát
        report.append(f"\nüáªüá≥ METRICS TI·∫æNG VI·ªÜT:")
        report.append(f"   ‚Ä¢ T·ª∑ l·ªá k√Ω t·ª± ti·∫øng Vi·ªát: {evaluation.vietnamese_chars_ratio:.1%}")
        report.append(f"   ‚Ä¢ T·ª´/c√¢u trung b√¨nh: {evaluation.avg_words_per_sentence:.1f}")
        report.append(f"   ‚Ä¢ S·ªë th·ª±c th·ªÉ: {evaluation.named_entities_count}")
        report.append(f"   ‚Ä¢ ƒêa d·∫°ng POS: {evaluation.pos_diversity:.2f}")
        
        if detailed:
            # Recommendations
            report.append(f"\nüí° KHUY·∫æN NGH·ªä:")
            for i, rec in enumerate(evaluation.recommendations, 1):
                report.append(f"   {i}. {rec}")
            
            # ƒê√°nh gi√° chi ti·∫øt
            report.append(f"\nüìã ƒê√ÅNH GI√Å CHI TI·∫æT:")
            
            # Quality assessment
            if evaluation.avg_coherence_score >= 0.8:
                report.append("   ‚úÖ Coherence r·∫•t t·ªët")
            elif evaluation.avg_coherence_score >= 0.6:
                report.append("   ‚ö†Ô∏è Coherence ch·∫•p nh·∫≠n ƒë∆∞·ª£c")
            else:
                report.append("   ‚ùå Coherence c·∫ßn c·∫£i thi·ªán")
            
            if evaluation.avg_completeness_score >= 0.8:
                report.append("   ‚úÖ Completeness r·∫•t t·ªët")
            elif evaluation.avg_completeness_score >= 0.7:
                report.append("   ‚ö†Ô∏è Completeness ch·∫•p nh·∫≠n ƒë∆∞·ª£c")
            else:
                report.append("   ‚ùå Completeness c·∫ßn c·∫£i thi·ªán")
            
            if evaluation.complete_sentences_ratio >= 0.8:
                report.append("   ‚úÖ C·∫•u tr√∫c c√¢u ƒë∆∞·ª£c b·∫£o to√†n t·ªët")
            else:
                report.append("   ‚ö†Ô∏è Nhi·ªÅu chunks b·ªã c·∫Øt gi·ªØa c√¢u")
            
            if evaluation.vietnamese_chars_ratio >= 0.7:
                report.append("   ‚úÖ VƒÉn b·∫£n ti·∫øng Vi·ªát ch·∫•t l∆∞·ª£ng cao")
            else:
                report.append("   ‚ö†Ô∏è Ki·ªÉm tra ch·∫•t l∆∞·ª£ng vƒÉn b·∫£n ti·∫øng Vi·ªát")
        
        report.append("\n" + "=" * 60)
        
        return "\n".join(report)
    
    def benchmark_strategies(self, 
                           text: str,
                           strategies: List[str] = None) -> Dict[str, Any]:
        """
        Benchmark nhi·ªÅu strategies tr√™n c√πng m·ªôt vƒÉn b·∫£n.
        
        Args:
            text: VƒÉn b·∫£n test
            strategies: Danh s√°ch t√™n strategies c·∫ßn test
            
        Returns:
            K·∫øt qu·∫£ benchmark
        """
        if strategies is None:
            strategies = ['fixed_size', 'sentence_aware', 'recursive', 'semantic']
        
        # Import chunkers (lazy import ƒë·ªÉ tr√°nh circular dependency)
        from .chunkers import VietnameseTextChunker, HybridVietnameseChunker
        from .chunking_strategies import (
            FixedSizeStrategy, SentenceAwareStrategy, 
            RecursiveStrategy, SemanticStrategy
        )
        
        strategy_map = {
            'fixed_size': FixedSizeStrategy(chunk_size=1000, overlap=200),
            'sentence_aware': SentenceAwareStrategy(target_size=1000),
            'recursive': RecursiveStrategy(chunk_size=1000, chunk_overlap=200),
            'semantic': SemanticStrategy(similarity_threshold=0.75)
        }
        
        results = {}
        evaluations = {}
        
        for strategy_name in strategies:
            if strategy_name not in strategy_map:
                self.logger.warning(f"Unknown strategy: {strategy_name}")
                continue
            
            try:
                start_time = time.time()
                
                # Chunk text
                strategy = strategy_map[strategy_name]
                chunker = VietnameseTextChunker(strategy)
                chunks = chunker.chunk_text(text)
                
                # Evaluate
                evaluation = self.evaluate_chunks(chunks, text)
                
                processing_time = time.time() - start_time
                
                results[strategy_name] = {
                    'chunks': chunks,
                    'evaluation': evaluation,
                    'processing_time': processing_time
                }
                evaluations[strategy_name] = evaluation
                
                self.logger.info(f"Benchmarked {strategy_name}: {len(chunks)} chunks, score: {evaluation.overall_quality_score:.2f}")
                
            except Exception as e:
                self.logger.error(f"Benchmark failed for {strategy_name}: {e}")
                continue
        
        # Compare results
        comparison = self.compare_strategies(evaluations)
        
        return {
            'individual_results': results,
            'comparison': comparison,
            'best_strategy': comparison.get('best_overall_strategy'),
            'benchmark_summary': {
                'total_strategies_tested': len(results),
                'best_score': comparison.get('best_overall_score', 0),
                'avg_score': comparison.get('summary_statistics', {}).get('avg_score', 0)
            }
        }