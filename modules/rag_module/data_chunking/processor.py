import json
import logging
import time
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# Chunking imports
from .chunkers import (
    VietnameseTextChunker,
    HybridVietnameseChunker,
    SemanticVietnameseChunker,
    RecursiveVietnameseChunker
)
from .chunking_strategies import (
    FixedSizeStrategy,
    SentenceAwareStrategy,
    SemanticStrategy,
    RecursiveStrategy
)
from .chunk_evaluator import ChunkQualityEvaluator
from .preprocessor import VietnameseTextPreprocessor
from .chunk_metadata import ChunkMetadata

# Setup logging
logger = logging.getLogger(__name__)

class DocumentType(Enum):
    """Ph√¢n lo·∫°i lo·∫°i t√†i li·ªáu."""
    ACADEMIC = "academic"       # H·ªçc thu·∫≠t, nghi√™n c·ª©u
    EDUCATIONAL = "educational" # Gi√°o d·ª•c, b√†i gi·∫£ng  
    TECHNICAL = "technical"     # K·ªπ thu·∫≠t, h∆∞·ªõng d·∫´n
    NARRATIVE = "narrative"     # T∆∞·ªùng thu·∫≠t, c√¢u chuy·ªán
    STRUCTURED = "structured"   # C√≥ c·∫•u tr√∫c r√µ r√†ng
    MIXED = "mixed"            # H·ªón h·ª£p nhi·ªÅu ki·ªÉu
    UNKNOWN = "unknown"        # Kh√¥ng x√°c ƒë·ªãnh

@dataclass
class StrategyPriority:
    """C·∫•u h√¨nh ∆∞u ti√™n strategy."""
    name: str
    priority: int              # 1 = cao nh·∫•t
    min_doc_size: int         # K√≠ch th∆∞·ªõc t√†i li·ªáu t·ªëi thi·ªÉu
    max_doc_size: int         # K√≠ch th∆∞·ªõc t√†i li·ªáu t·ªëi ƒëa
    suitable_types: List[DocumentType]
    min_quality: float        # ƒêi·ªÉm ch·∫•t l∆∞·ª£ng t·ªëi thi·ªÉu
    default_params: Dict[str, Any]

class IntelligentVietnameseChunkingProcessor:
    """
    Processor th√¥ng minh v·ªõi c∆° ch·∫ø ∆∞u ti√™n v√† fallback.
    
    Workflow:
    1. Ph√¢n t√≠ch vƒÉn b·∫£n ‚Üí x√°c ƒë·ªãnh lo·∫°i t√†i li·ªáu
    2. Th·ª≠ strategies theo th·ª© t·ª± ∆∞u ti√™n
    3. ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng ‚Üí fallback n·∫øu kh√¥ng ƒë·∫°t
    4. Tr·∫£ v·ªÅ k·∫øt qu·∫£ t·ªët nh·∫•t
    """
    
    def __init__(self, output_dir: str = "chunking_output", min_quality: float = 0.65):
        """
        Args:
            output_dir: Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
            min_quality: ƒêi·ªÉm ch·∫•t l∆∞·ª£ng t·ªëi thi·ªÉu ch·∫•p nh·∫≠n
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.min_quality = min_quality
        
        # T·∫°o evaluator
        self.evaluator = ChunkQualityEvaluator(
            min_acceptable_coherence=0.6,
            min_acceptable_completeness=0.7,
            target_chunk_size=1000
        )
        
        # Preprocessor
        self.preprocessor = VietnameseTextPreprocessor(
            normalize_text=True,
            remove_extra_whitespace=True,
            preserve_structure=True
        )
        
        # C·∫•u h√¨nh strategies theo th·ª© t·ª± ∆∞u ti√™n
        self.strategy_priorities = [
            StrategyPriority(
                name="hybrid",
                priority=1,
                min_doc_size=1000,
                max_doc_size=100000,
                suitable_types=[DocumentType.MIXED, DocumentType.EDUCATIONAL, DocumentType.ACADEMIC],
                min_quality=0.65,
                default_params={
                    'selection_criteria': 'balanced',
                    'chunk_size': 1000,
                    'overlap_ratio': 0.2
                }
            ),
            StrategyPriority(
                name="semantic",
                priority=2,
                min_doc_size=2000,
                max_doc_size=50000,
                suitable_types=[DocumentType.ACADEMIC, DocumentType.EDUCATIONAL, DocumentType.TECHNICAL],
                min_quality=0.75,
                default_params={
                    'embedding_model': 'all-MiniLM-L6-v2',
                    'similarity_threshold': 0.75,
                    'adaptive_threshold': True
                }
            ),
            StrategyPriority(
                name="recursive",
                priority=3,
                min_doc_size=500,
                max_doc_size=200000,
                suitable_types=[DocumentType.STRUCTURED, DocumentType.TECHNICAL, DocumentType.NARRATIVE],
                min_quality=0.6,
                default_params={
                    'base_chunk_size': 1000,
                    'overlap_ratio': 0.2,
                    'adaptive_sizing': True,
                    'preserve_sentences': True
                }
            ),
            StrategyPriority(
                name="sentence",
                priority=4,
                min_doc_size=300,
                max_doc_size=50000,
                suitable_types=[DocumentType.EDUCATIONAL, DocumentType.NARRATIVE],
                min_quality=0.55,
                default_params={
                    'target_size': 800,
                    'max_sentences': 8
                }
            ),
            StrategyPriority(
                name="fixed",
                priority=5,              # Fallback cu·ªëi c√πng
                min_doc_size=100,
                max_doc_size=1000000,    # Lu√¥n ho·∫°t ƒë·ªông
                suitable_types=list(DocumentType),
                min_quality=0.4,
                default_params={
                    'chunk_size': 1000,
                    'overlap': 200
                }
            )
        ]
        
        logger.info(f"‚úÖ IntelligentProcessor initialized with min_quality: {self.min_quality}")
        logger.info(f"   ‚Ä¢ Strategies configured: {len(self.strategy_priorities)}")
    
    def read_file(self, file_path: Path) -> str:
        """ƒê·ªçc n·ªôi dung file markdown."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if not content.strip():
                raise ValueError("File r·ªóng ho·∫∑c ch·ªâ ch·ª©a whitespace")
            
            logger.info(f"üìñ ƒê·ªçc file th√†nh c√¥ng: {file_path.name} ({len(content):,} k√Ω t·ª±)")
            return content
            
        except FileNotFoundError:
            logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {file_path}")
            raise
        except UnicodeDecodeError:
            logger.error(f"‚ùå L·ªói encoding file: {file_path}")
            raise
        except Exception as e:
            logger.error(f"‚ùå L·ªói ƒë·ªçc file {file_path}: {e}")
            raise
    
    def analyze_document(self, content: str) -> Tuple[DocumentType, Dict[str, Any]]:
        """
        Ph√¢n t√≠ch t√†i li·ªáu ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i v√† ƒë·∫∑c ƒëi·ªÉm.
        
        Args:
            content: N·ªôi dung t√†i li·ªáu
            
        Returns:
            Tuple (DocumentType, analysis_info)
        """
        logger.info("üîç Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm t√†i li·ªáu...")
        
        # Th·ªëng k√™ c∆° b·∫£n
        stats = self.preprocessor.get_text_statistics(content)
        vietnamese_ratio = self.preprocessor.detect_language_confidence(content)
        
        # Ph√¢n t√≠ch c·∫•u tr√∫c
        has_headers = bool(re.search(r'^#+\s', content, re.MULTILINE))
        has_lists = bool(re.search(r'^[\s]*[-*+]\s|^\s*\d+\.\s', content, re.MULTILINE))
        has_code = bool(re.search(r'```|`[^`]+`', content))
        
        # Ph√°t hi·ªán thu·∫≠t ng·ªØ k·ªπ thu·∫≠t
        technical_indicators = len(re.findall(r'[A-Z]{2,}|[a-z]+\([^)]*\)|[Œ±-œâ]|\d+\.\d+|%|\$', content))
        has_technical = technical_indicators > len(content) * 0.01
        
        # T√≠nh ƒëi·ªÉm ph·ª©c t·∫°p
        complexity_score = min(1.0, (
            min(stats.avg_words_per_sentence / 20, 1.0) * 0.3 +
            min(technical_indicators / 100, 1.0) * 0.3 +
            min(stats.paragraph_count / 50, 1.0) * 0.2 +
            (1.0 - vietnamese_ratio) * 0.2
        ))
        
        # T√≠nh ƒëi·ªÉm c·∫•u tr√∫c  
        structure_score = (
            (1.0 if has_headers else 0.0) * 0.4 +
            (1.0 if has_lists else 0.0) * 0.3 +
            (1.0 if has_code else 0.0) * 0.1 +
            min(stats.paragraph_count / 20, 1.0) * 0.2
        )
        
        # X√°c ƒë·ªãnh lo·∫°i t√†i li·ªáu
        doc_type = self._classify_document_type(
            stats, complexity_score, structure_score, 
            has_headers, has_lists, has_technical, vietnamese_ratio
        )
        
        analysis_info = {
            'doc_type': doc_type,
            'complexity_score': complexity_score,
            'structure_score': structure_score,
            'vietnamese_ratio': vietnamese_ratio,
            'avg_sentence_length': stats.avg_words_per_sentence,
            'paragraph_count': stats.paragraph_count,
            'has_headers': has_headers,
            'has_lists': has_lists,
            'has_technical': has_technical,
            'file_size': len(content)
        }
        
        logger.info(f"üìä Ph√¢n t√≠ch ho√†n t·∫•t:")
        logger.info(f"   ‚Ä¢ Lo·∫°i t√†i li·ªáu: {doc_type.value}")
        logger.info(f"   ‚Ä¢ ƒê·ªô ph·ª©c t·∫°p: {complexity_score:.2f}")
        logger.info(f"   ‚Ä¢ C·∫•u tr√∫c: {structure_score:.2f}")
        logger.info(f"   ‚Ä¢ Ti·∫øng Vi·ªát: {vietnamese_ratio:.1%}")
        
        return doc_type, analysis_info
    
    def _classify_document_type(self, stats, complexity_score, structure_score,
                               has_headers, has_lists, has_technical, vietnamese_ratio) -> DocumentType:
        """Ph√¢n lo·∫°i lo·∫°i t√†i li·ªáu."""
        
        # Academic: ph·ª©c t·∫°p, c√≥ c·∫•u tr√∫c, thu·∫≠t ng·ªØ k·ªπ thu·∫≠t
        if (complexity_score > 0.6 and structure_score > 0.5 and 
            has_technical and has_headers):
            return DocumentType.ACADEMIC
        
        # Educational: c·∫•u tr√∫c t·ªët, ƒë·ªô ph·ª©c t·∫°p v·ª´a
        if (structure_score > 0.6 and 0.3 < complexity_score < 0.7 and
            (has_headers or has_lists)):
            return DocumentType.EDUCATIONAL
        
        # Technical: c√≥ thu·∫≠t ng·ªØ k·ªπ thu·∫≠t, c√≥ c·∫•u tr√∫c
        if has_technical and structure_score > 0.4:
            return DocumentType.TECHNICAL
        
        # Structured: c√≥ headers, lists, c·∫•u tr√∫c r√µ r√†ng
        if structure_score > 0.7 and (has_headers and has_lists):
            return DocumentType.STRUCTURED
        
        # Narrative: c√¢u d√†i, √≠t c·∫•u tr√∫c, nhi·ªÅu ti·∫øng Vi·ªát
        if (stats.avg_words_per_sentence > 15 and structure_score < 0.4 and
            vietnamese_ratio > 0.8):
            return DocumentType.NARRATIVE
        
        # Mixed: trung b√¨nh v·ªÅ c√°c ch·ªâ s·ªë
        if 0.3 < complexity_score < 0.7 and 0.3 < structure_score < 0.7:
            return DocumentType.MIXED
        
        return DocumentType.UNKNOWN
    
    def get_prioritized_strategies(self, doc_type: DocumentType, doc_size: int) -> List[StrategyPriority]:
        """L·∫•y danh s√°ch strategies theo th·ª© t·ª± ∆∞u ti√™n."""
        
        suitable_strategies = []
        
        for strategy_config in self.strategy_priorities:
            # Ki·ªÉm tra ph√π h·ª£p v·ªõi lo·∫°i t√†i li·ªáu v√† k√≠ch th∆∞·ªõc
            if strategy_config.min_doc_size <= doc_size <= strategy_config.max_doc_size:
                suitable_strategies.append(strategy_config)
        
        # S·∫Øp x·∫øp theo priority (1 = cao nh·∫•t)
        suitable_strategies.sort(key=lambda x: x.priority)
        
        # Lu√¥n c√≥ √≠t nh·∫•t fixed strategy
        if not suitable_strategies:
            fixed_strategy = next(s for s in self.strategy_priorities if s.name == "fixed")
            suitable_strategies = [fixed_strategy]
        
        return suitable_strategies
    
    def create_chunker_from_config(self, strategy_config: StrategyPriority, custom_params: Dict = None):
        """T·∫°o chunker t·ª´ config."""
        
        params = strategy_config.default_params.copy()
        if custom_params:
            params.update(custom_params)
        
        strategy_name = strategy_config.name
        
        if strategy_name == 'semantic':
            return SemanticVietnameseChunker(
                preprocessor=self.preprocessor,
                **params
            )
        
        elif strategy_name == 'hybrid':
            chunk_size = params.get('chunk_size', 1000)
            overlap_ratio = params.get('overlap_ratio', 0.2)
            
            strategies = [
                FixedSizeStrategy(chunk_size=chunk_size, overlap=int(chunk_size * overlap_ratio)),
                SentenceAwareStrategy(target_size=chunk_size),
                RecursiveStrategy(chunk_size=chunk_size, chunk_overlap=int(chunk_size * overlap_ratio))
            ]
            
            return HybridVietnameseChunker(
                strategies=strategies,
                selection_criteria=params.get('selection_criteria', 'balanced'),
                preprocessor=self.preprocessor
            )
        
        elif strategy_name == 'recursive':
            return RecursiveVietnameseChunker(
                preprocessor=self.preprocessor,
                **params
            )
        
        elif strategy_name == 'sentence':
            return VietnameseTextChunker(
                SentenceAwareStrategy(
                    target_size=params.get('target_size', 800),
                    max_sentences=params.get('max_sentences', 8)
                ),
                preprocessor=self.preprocessor
            )
        
        elif strategy_name == 'fixed':
            return VietnameseTextChunker(
                FixedSizeStrategy(
                    chunk_size=params.get('chunk_size', 1000),
                    overlap=params.get('overlap', 200)
                ),
                preprocessor=self.preprocessor
            )
        
        else:
            raise ValueError(f"Unknown strategy: {strategy_name}")
    
    def process_chunking(self, file_path: Path, custom_strategy: str = None, **kwargs) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω chunking th√¥ng minh v·ªõi ∆∞u ti√™n strategies.
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n file
            custom_strategy: Strategy c·ª• th·ªÉ (b·ªè qua intelligent n·∫øu c√≥)
            **kwargs: Parameters t√πy ch·ªânh
            
        Returns:
            K·∫øt qu·∫£ chunking v√† th√¥ng tin attempts
        """
        start_time = time.time()
        
        logger.info(f"üß† B·∫Øt ƒë·∫ßu intelligent chunking: {file_path.name}")
        
        # ƒê·ªçc file
        content = self.read_file(file_path)
        
        # N·∫øu c√≥ custom strategy, s·ª≠ d·ª•ng tr·ª±c ti·∫øp
        if custom_strategy:
            logger.info(f"üéØ S·ª≠ d·ª•ng custom strategy: {custom_strategy}")
            return self._process_single_strategy(content, file_path, custom_strategy, start_time, **kwargs)
        
        # Ph√¢n t√≠ch t√†i li·ªáu
        doc_type, analysis_info = self.analyze_document(content)
        
        # L·∫•y strategies theo th·ª© t·ª± ∆∞u ti√™n
        prioritized_strategies = self.get_prioritized_strategies(doc_type, len(content))
        
        logger.info(f"üìã Strategies s·∫Ω th·ª≠ theo th·ª© t·ª±:")
        for i, strategy in enumerate(prioritized_strategies, 1):
            logger.info(f"   {i}. {strategy.name} (min_quality: {strategy.min_quality})")
        
        # Th·ª≠ t·ª´ng strategy theo th·ª© t·ª± ∆∞u ti√™n
        attempts = []
        best_result = None
        
        for attempt_num, strategy_config in enumerate(prioritized_strategies, 1):
            logger.info(f"üîÑ Attempt {attempt_num}: Trying {strategy_config.name}")
            
            try:
                # T·∫°o chunker
                chunker = self.create_chunker_from_config(strategy_config, kwargs)
                
                # Chunking
                chunk_start_time = time.time()
                chunks = chunker.chunk_text(content)  # Kh√¥ng truy·ªÅn source_info
                chunk_time = time.time() - chunk_start_time
                
                if not chunks:
                    logger.warning(f"‚ö†Ô∏è  {strategy_config.name} produced no chunks")
                    continue
                
                # ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng
                evaluation = self.evaluator.evaluate_chunks(chunks, content)
                quality_score = evaluation.overall_quality_score
                
                attempt_info = {
                    'strategy': strategy_config.name,
                    'params': strategy_config.default_params,
                    'chunks_count': len(chunks),
                    'quality_score': quality_score,
                    'processing_time': chunk_time,
                    'success': quality_score >= strategy_config.min_quality,
                    'chunks': chunks,
                    'evaluation': evaluation
                }
                
                attempts.append(attempt_info)
                
                logger.info(f"   ‚Ä¢ Quality: {quality_score:.2f} (c·∫ßn: {strategy_config.min_quality:.2f})")
                logger.info(f"   ‚Ä¢ Chunks: {len(chunks)}")
                logger.info(f"   ‚Ä¢ Time: {chunk_time:.2f}s")
                
                # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng
                if quality_score >= strategy_config.min_quality:
                    logger.info(f"‚úÖ {strategy_config.name} PASSED - s·ª≠ d·ª•ng strategy n√†y!")
                    best_result = attempt_info
                    break
                else:
                    logger.info(f"‚ùå {strategy_config.name} FAILED - th·ª≠ strategy ti·∫øp theo")
                    
            except Exception as e:
                logger.error(f"‚ùå {strategy_config.name} error: {e}")
                attempts.append({
                    'strategy': strategy_config.name,
                    'error': str(e),
                    'success': False
                })
                continue
        
        # N·∫øu kh√¥ng c√≥ strategy n√†o pass, ch·ªçn c√°i t·ªët nh·∫•t
        if not best_result and attempts:
            successful_attempts = [a for a in attempts if 'quality_score' in a]
            if successful_attempts:
                best_result = max(successful_attempts, key=lambda x: x['quality_score'])
                logger.warning(f"‚ö†Ô∏è  Kh√¥ng c√≥ strategy n√†o ƒë·∫°t y√™u c·∫ßu, ch·ªçn t·ªët nh·∫•t: {best_result['strategy']}")
            else:
                logger.error("‚ùå T·∫•t c·∫£ strategies ƒë·ªÅu th·∫•t b·∫°i ho√†n to√†n")
        
        if not best_result:
            raise RuntimeError("‚ùå T·∫•t c·∫£ strategies ƒë·ªÅu th·∫•t b·∫°i!")
        
        total_time = time.time() - start_time
        
        # T·∫°o k·∫øt qu·∫£ cu·ªëi c√πng
        result = self._build_final_result(
            file_path, content, analysis_info, best_result, attempts, total_time
        )
        
        logger.info(f"üéØ Selected: {best_result['strategy']} (Quality: {best_result['quality_score']:.2f})")
        
        return result
    
    def _process_single_strategy(self, content: str, file_path: Path, strategy_name: str, 
                               start_time: float, **kwargs) -> Dict[str, Any]:
        """X·ª≠ l√Ω v·ªõi m·ªôt strategy c·ª• th·ªÉ."""
        
        # T√¨m config strategy
        strategy_config = next((s for s in self.strategy_priorities if s.name == strategy_name), None)
        if not strategy_config:
            raise ValueError(f"Strategy '{strategy_name}' kh√¥ng t·ªìn t·∫°i")
        
        # T·∫°o chunker
        chunker = self.create_chunker_from_config(strategy_config, kwargs)
        
        # Chunking
        chunks = chunker.chunk_text(content)
        
        if not chunks:
            raise RuntimeError(f"Strategy {strategy_name} kh√¥ng t·∫°o ra chunks n√†o")
        
        # ƒê√°nh gi√°
        evaluation = self.evaluator.evaluate_chunks(chunks, content)
        total_time = time.time() - start_time
        
        # Fake analysis info
        analysis_info = {
            'doc_type': DocumentType.UNKNOWN,
            'complexity_score': 0.5,
            'structure_score': 0.5,
            'file_size': len(content)
        }
        
        result_info = {
            'strategy': strategy_name,
            'chunks_count': len(chunks),
            'quality_score': evaluation.overall_quality_score,
            'chunks': chunks,
            'evaluation': evaluation,
            'success': True
        }
        
        return self._build_final_result(
            file_path, content, analysis_info, result_info, [result_info], total_time
        )
    
    def _build_final_result(self, file_path: Path, content: str, analysis_info: Dict,
                           best_result: Dict, attempts: List, total_time: float) -> Dict[str, Any]:
        """T·∫°o k·∫øt qu·∫£ cu·ªëi c√πng."""
        
        return {
            'input_info': {
                'file_path': str(file_path),
                'file_name': file_path.name,
                'file_size': len(content),
                'character_count': len(content),
                'line_count': content.count('\n') + 1,
                'strategy': best_result['strategy']
            },
            
            'document_analysis': {
                **analysis_info,
                'doc_type': analysis_info['doc_type'].value if hasattr(analysis_info.get('doc_type'), 'value') else str(analysis_info.get('doc_type', 'unknown'))
            },
            
            'intelligent_process': {
                'selected_strategy': best_result['strategy'],
                'total_attempts': len(attempts),
                'success_on_attempt': next((i+1 for i, a in enumerate(attempts) if a.get('success')), len(attempts)),
                'total_processing_time': total_time
            },
            
            'chunking_results': {
                'total_chunks': best_result['chunks_count'],
                'chunking_time': best_result.get('processing_time', total_time),
                'total_processing_time': total_time,
                'chunks_data': [chunk.to_dict() for chunk in best_result['chunks']]
            },
            
            'quality_evaluation': {
                'overall_score': best_result['evaluation'].overall_quality_score,
                'coherence_score': best_result['evaluation'].avg_coherence_score,
                'completeness_score': best_result['evaluation'].avg_completeness_score,
                'language_confidence': best_result['evaluation'].avg_language_confidence,
                'chunk_size_stats': {
                    'avg_size': best_result['evaluation'].avg_chunk_size,
                    'min_size': best_result['evaluation'].min_chunk_size,
                    'max_size': best_result['evaluation'].max_chunk_size,
                    'std_size': best_result['evaluation'].std_chunk_size
                },
                'structure_metrics': {
                    'complete_sentences_ratio': best_result['evaluation'].complete_sentences_ratio,
                    'paragraph_preservation': best_result['evaluation'].paragraph_preservation_ratio,
                    'overlap_efficiency': best_result['evaluation'].overlap_efficiency
                },
                'vietnamese_metrics': {
                    'vietnamese_chars_ratio': best_result['evaluation'].vietnamese_chars_ratio,
                    'avg_words_per_sentence': best_result['evaluation'].avg_words_per_sentence,
                    'pos_diversity': best_result['evaluation'].pos_diversity
                },
                'recommendations': best_result['evaluation'].recommendations
            },
            
            'all_attempts': [
                {
                    'strategy': att.get('strategy'),
                    'success': att.get('success', False),
                    'quality_score': att.get('quality_score', 0.0),
                    'chunks_count': att.get('chunks_count', 0),
                    'error': att.get('error')
                }
                for att in attempts
            ],
            
            'metadata': {
                'processing_timestamp': datetime.now().isoformat(),
                'processor_version': "2.0.0",
                'intelligence_enabled': True
            }
        }
    
    def print_report(self, result: Dict[str, Any]):
        """In b√°o c√°o intelligent chunking."""
        
        print("\n" + "=" * 80)
        print("üß† B√ÅO C√ÅO INTELLIGENT CHUNKING")
        print("=" * 80)
        
        # Input info
        input_info = result['input_info']
        print(f"\nüìÅ TH√îNG TIN FILE:")
        print(f"   ‚Ä¢ File: {input_info['file_name']}")
        print(f"   ‚Ä¢ K√≠ch th∆∞·ªõc: {input_info['file_size']:,} k√Ω t·ª±")
        print(f"   ‚Ä¢ Strategy ƒë∆∞·ª£c ch·ªçn: {input_info['strategy']}")
        
        # Document analysis
        if 'document_analysis' in result:
            analysis = result['document_analysis']
            print(f"\nüîç PH√ÇN T√çCH T√ÄI LI·ªÜU:")
            print(f"   ‚Ä¢ Lo·∫°i: {analysis.get('doc_type', 'unknown')}")
            print(f"   ‚Ä¢ ƒê·ªô ph·ª©c t·∫°p: {analysis.get('complexity_score', 0):.2f}")
            print(f"   ‚Ä¢ C·∫•u tr√∫c: {analysis.get('structure_score', 0):.2f}")
        
        # Intelligence process
        if 'intelligent_process' in result:
            intel = result['intelligent_process']
            print(f"\nüß† QU√Å TR√åNH TH√îNG MINH:")
            print(f"   ‚Ä¢ S·ªë attempts: {intel['total_attempts']}")
            print(f"   ‚Ä¢ Th√†nh c√¥ng ·ªü attempt: {intel['success_on_attempt']}")
            print(f"   ‚Ä¢ Th·ªùi gian t·ªïng: {intel['total_processing_time']:.2f}s")
            
            if intel['success_on_attempt'] == 1:
                print("   üéØ PERFECT! Strategy ƒë·∫ßu ti√™n ƒë√£ th√†nh c√¥ng!")
            elif intel['success_on_attempt'] <= 2:
                print("   ‚úÖ GOOD! Nhanh ch√≥ng t√¨m ƒë∆∞·ª£c strategy ph√π h·ª£p")
            else:
                print("   ‚ö†Ô∏è OK - C·∫ßn nhi·ªÅu attempts")
        
        # Results
        chunking = result['chunking_results']
        quality = result['quality_evaluation']
        
        print(f"\n‚ö° K·∫æT QU·∫¢:")
        print(f"   ‚Ä¢ Chunks: {chunking['total_chunks']}")
        print(f"   ‚Ä¢ Quality: {quality['overall_score']:.2f}/1.00")
        print(f"   ‚Ä¢ Coherence: {quality['coherence_score']:.2f}")
        print(f"   ‚Ä¢ Completeness: {quality['completeness_score']:.2f}")
        
        # Attempts summary
        if 'all_attempts' in result:
            print(f"\nüìä T·∫§T C·∫¢ ATTEMPTS:")
            for i, attempt in enumerate(result['all_attempts'], 1):
                status = "‚úÖ" if attempt['success'] else "‚ùå"
                quality_str = f"{attempt['quality_score']:.2f}" if attempt['quality_score'] > 0 else "N/A"
                print(f"   {i}. {attempt['strategy']}: {status} (Quality: {quality_str})")
        
        # Sample chunks
        chunks_data = chunking['chunks_data']
        print(f"\nüìÑ M·∫™U CHUNKS (2 chunks ƒë·∫ßu):")
        for i, chunk_data in enumerate(chunks_data[:2], 1):
            preview = chunk_data['content'][:100] + "..." if len(chunk_data['content']) > 100 else chunk_data['content']
            print(f"\n   Chunk {i} ({chunk_data['char_count']} chars):")
            print(f"     ‚Ä¢ Keywords: {', '.join(chunk_data['keywords'][:3])}")
            print(f"     ‚Ä¢ Preview: {preview}")
        
        print(f"\nüéØ ƒê√ÅNH GI√Å:")
        if quality['overall_score'] >= 0.8:
            print("   üåü XU·∫§T S·∫ÆC - Intelligent chunking ho·∫°t ƒë·ªông t·ªëi ∆∞u!")
        elif quality['overall_score'] >= 0.7:
            print("   ‚úÖ T·ªêT - Intelligent chunking hi·ªáu qu·∫£")
        elif quality['overall_score'] >= 0.6:
            print("   ‚ö†Ô∏è CH·∫§P NH·∫¨N ƒê∆Ø·ª¢C - C√≥ th·ªÉ c·∫ßn fine-tuning")
        else:
            print("   üîß C·∫¶N C·∫¢I THI·ªÜN - T√†i li·ªáu c√≥ ƒë·∫∑c ƒëi·ªÉm ph·ª©c t·∫°p")
        
        print("\n" + "=" * 80)
    
    def save_json_results(self, result: Dict[str, Any], file_path: Path) -> Dict[str, str]:
        """
        L∆∞u k·∫øt qu·∫£ ra file JSON - ch·ªâ chunks thu·∫ßn.
        
        Args:
            result: K·∫øt qu·∫£ chunking
            file_path: ƒê∆∞·ªùng d·∫´n file g·ªëc
            
        Returns:
            Dict v·ªõi ƒë∆∞·ªùng d·∫´n file ƒë√£ l∆∞u
        """
        # T·∫°o t√™n file output
        input_filename = file_path.stem
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        strategy = result['input_info']['strategy']
        
        # File chunks thu·∫ßn cho embedding
        chunks_filename = f"{input_filename}_{strategy}_{timestamp}_chunks.json"
        chunks_path = self.output_dir / chunks_filename
        
        try:
            # T·∫°o chunks array thu·∫ßn - ch·ªâ chunks, kh√¥ng c√≥ metadata th·ª´a
            chunks_data = []
            
            for chunk_data in result['chunking_results']['chunks_data']:
                chunk_clean = {
                    'id': chunk_data['chunk_id'],
                    'content': chunk_data['content'],
                    'metadata': {
                        'chunk_index': chunk_data['chunk_index'],
                        'char_count': chunk_data['char_count'],
                        'word_count': chunk_data['word_count'],
                        'keywords': chunk_data['keywords'],
                        'coherence_score': chunk_data['semantic_coherence_score'],
                        'completeness_score': chunk_data['completeness_score'],
                        'language_confidence': chunk_data['language_confidence'],
                        'chunking_strategy': chunk_data['chunking_strategy']
                    }
                }
                chunks_data.append(chunk_clean)
            
            # L∆∞u ch·ªâ m·∫£ng chunks
            with open(chunks_path, 'w', encoding='utf-8') as f:
                json.dump(chunks_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"üíæ ƒê√£ l∆∞u chunks: {chunks_path}")
            
            return {
                'chunks_json': str(chunks_path)
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói l∆∞u file: {e}")
            raise
    
    def run(self, file_path: Path, strategy: str = None, 
            save_json: bool = True, print_report: bool = True, **kwargs) -> Dict[str, Any]:
        """
        Ch·∫°y intelligent chunking ho√†n ch·ªânh.
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n file markdown
            strategy: Strategy c·ª• th·ªÉ (None = intelligent auto-select)
            save_json: C√≥ l∆∞u k·∫øt qu·∫£ JSON kh√¥ng
            print_report: C√≥ in b√°o c√°o kh√¥ng
            **kwargs: Parameters t√πy ch·ªânh cho strategies
            
        Returns:
            K·∫øt qu·∫£ chunking v√† ƒë∆∞·ªùng d·∫´n files ƒë√£ l∆∞u
        """
        try:
            # X·ª≠ l√Ω chunking (intelligent ho·∫∑c single strategy)
            result = self.process_chunking(file_path, custom_strategy=strategy, **kwargs)
            
            # In b√°o c√°o
            if print_report:
                self.print_report(result)
            
            # L∆∞u JSON
            saved_files = {}
            if save_json:
                saved_files = self.save_json_results(result, file_path)
                print(f"\nüíæ FILE ƒê√É L∆ØU:")
                print(f"   ‚Ä¢ Chunks: {saved_files['chunks_json']}")
            
            return {
                'result': result,
                'saved_files': saved_files
            }
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói intelligent processing: {e}")
            raise

# Alias cho backward compatibility
VietnameseChunkingProcessor = IntelligentVietnameseChunkingProcessor